{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load dataset\n",
    "df = pd.read_csv(r'C:\\Users\\ksvsu\\OneDrive\\Desktop\\Minor Project\\Tweets.csv',\n",
    "                 encoding='ISO-8859-1', header=None, on_bad_lines='skip')\n",
    "df.columns = ['target', 'id', 'date', 'text']  # Adjust according to the actual number of columns\n",
    "\n",
    "# Step 3: Simplify target labels (0 = negative, 4 = positive to binary 0 and 1)\n",
    "df['target'] = df['target'].apply(lambda x: 1 if x == 4 else 0)\n",
    "\n",
    "# Step 4: Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text data using the BERT tokenizer\n",
    "def tokenize_tweets(tweets, tokenizer, max_length=128):\n",
    "    return tokenizer(\n",
    "        tweets.tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "tokens = tokenize_tweets(df['text'], tokenizer)\n",
    "\n",
    "# Prepare inputs for TensorFlow\n",
    "input_ids = tokens['input_ids']\n",
    "attention_masks = tokens['attention_mask']\n",
    "\n",
    "# **Convert TensorFlow tensors to NumPy arrays**\n",
    "input_ids_np = input_ids.numpy()\n",
    "attention_masks_np = attention_masks.numpy()\n",
    "targets_np = df['target'].values\n",
    "\n",
    "# **Step 5: Split the data into training and validation sets together**\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    input_ids_np, attention_masks_np, targets_np,\n",
    "    test_size=0.2, random_state=42, stratify=targets_np\n",
    ")\n",
    "\n",
    "# Step 6: Define a function to create TensorFlow datasets\n",
    "def create_tf_dataset(inputs, masks, labels, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {'input_ids': inputs, 'attention_mask': masks}, labels))\n",
    "    return dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = create_tf_dataset(train_inputs, train_masks, train_labels, batch_size)\n",
    "val_dataset = create_tf_dataset(val_inputs, val_masks, val_labels, batch_size)\n",
    "\n",
    "# Step 7: Create a custom BERT model with additional layers\n",
    "class CustomBERTModel(tf.keras.Model):\n",
    "    def __init__(self, base_model):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.dropout = Dropout(0.3)\n",
    "        self.classifier = Dense(2, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.base_model(inputs)\n",
    "        pooled_output = outputs.pooler_output  # Use the pooled output for classification\n",
    "        x = self.dropout(pooled_output, training=training)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Load pre-trained BERT and attach custom layers\n",
    "base_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model = CustomBERTModel(base_model.bert)\n",
    "\n",
    "# Step 8: Define loss, optimizer, and metrics\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = Adam(learning_rate=3e-5)  # Use a standard learning rate for fine-tuning BERT\n",
    "\n",
    "# Step 9: Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Step 10: Implement early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Step 11: Train the model with early stopping\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 12: Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(val_dataset)\n",
    "print(f\"Final Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Step 13: Plot the training and validation accuracy and loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 11: Train the model with early stopping\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      3\u001b[0m     train_dataset,\n\u001b[0;32m      4\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      5\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[0;32m      6\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping],\n\u001b[0;32m      7\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Increase verbosity to get more details in the output\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 12: Evaluate the model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(val_dataset)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 11: Train the model with early stopping\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2  # Increase verbosity to get more details in the output\n",
    ")\n",
    "\n",
    "# Step 12: Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(val_dataset)\n",
    "print(f\"Final Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Clear TensorFlow session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Print confirmation to check code progression\n",
    "print(\"Model evaluation complete. Proceeding to plotting.\")\n",
    "\n",
    "# Step 13: Plot the training and validation accuracy and loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
